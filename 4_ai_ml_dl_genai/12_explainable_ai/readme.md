---
title: "Explainable AI (XAI)"
category: good-to-know
levels: ["senior", "principal"]
skills: [xai, shap, lime]
questions:
  "mid": ["What is feature importance?"]
  "senior": ["Explain how SHAP values are calculated."]
---

# Explainable AI (XAI)

## Core Ideas
- Global vs Local Interpretability.
- SHAP (Shapley Additive Explanations).
- LIME (Local Interpretable Model-agnostic Explanations).
- Saliency Maps.

## Resources
- [Interpretable Machine Learning (Molnar)](https://christophm.github.io/interpretable-ml-book/)

## Practice
- Use SHAP to explain a Random Forest model's predictions.
- Generate saliency maps for a CNN.

## Questions
### Mid Level
- Why is interpretability important in regulated industries?

### Senior Level
- Discuss the limitations of LIME.