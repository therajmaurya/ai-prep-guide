---
title: "Deep Learning Fundamentals"
category: must
levels: ["mid", "senior"]
skills: [deep-learning, neural-networks, backpropagation]
questions:
  "mid": ["Explain the Vanishing Gradient problem."]
  "senior": ["Compare different weight initialization techniques (Xavier vs He)."]
---

# Deep Learning Fundamentals

## Core Ideas
- Perceptrons and MLPs.
- Activation Functions (ReLU, Sigmoid, Tanh).
- Backpropagation and Optimizers (SGD, Adam).
- Batch Normalization and Dropout.

## Resources
- [Deep Learning Book (Goodfellow)](https://www.deeplearningbook.org/)
- [Fast.ai](https://www.fast.ai/)

## Practice
- Build a simple MLP for MNIST classification.
- Visualize the effect of different learning rates.

## Questions
### Mid Level
- Why do we need non-linear activation functions?

### Senior Level
- Explain the concept of a computational graph.
