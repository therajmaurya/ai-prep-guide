---
title: "Optimization for ML"
category: must
levels: ["senior", "principal"]
skills: [optimization, convex-optimization, sgd]
questions:
  "mid": ["Explain Stochastic Gradient Descent."]
  "senior": ["How does momentum help in optimization?"]
---

# Optimization for ML

## Core Ideas
- Convex vs Non-Convex Optimization.
- First-order vs Second-order methods.
- Learning Rate Schedules.
- Regularization effects on optimization.

## Resources
- [Convex Optimization (Boyd)](https://web.stanford.edu/~boyd/cvxbook/)

## Practice
- Implement Adam optimizer from scratch.
- Visualize the loss landscape of a simple neural network.

## Questions
### Mid Level
- What is the role of the learning rate?

### Senior Level
- Discuss the convergence properties of SGD.
